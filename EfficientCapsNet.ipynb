{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saqlainkazi690/Projects/blob/main/EfficientCapsNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D40sxes5MreT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 17"
      ],
      "metadata": {
        "id": "OxLr_l3ldVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvLayer, self).__init__() # input is MURA (batch, 3, 224, 224), MNIST (batch, 3, 28, 28)\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 7, stride = 2, padding = 0) # (batch, 32, 100, 109), (batch, 32, 24, 24)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 2, padding = 0) #(batch, 64, 52, 52), (batch, 64, 22, 22)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 5, stride = 2, padding = 0) # (batch, 64, 24, 24), (batch, 64, 20, 20)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 5, stride = 2, padding = 0) #(batch, 128, 10, 10), (batch, 128, 9, 9)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(num_features = 32)\n",
        "    self.bn2 = nn.BatchNorm2d(num_features = 64)\n",
        "    self.bn3 = nn.BatchNorm2d(num_features = 64)\n",
        "    self.bn4 = nn.BatchNorm2d(num_features = 128)\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "    return x\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "\n",
        "class DepthWiseConv(nn.Module):\n",
        "  def __init__(self,capsule_size = (16, 8), kernel_size = 11):\n",
        "    super(DepthWiseConv, self).__init__()\n",
        "    self.num_capsules, self.dim_capsules = capsule_size\n",
        "    self.dwc = nn.Conv2d(in_channels = 128, out_channels = self.num_capsules * self.dim_capsules, kernel_size = kernel_size, groups = 128 , padding = 0)\n",
        "    # Output shape is (batch_size, 128, 1, 1)\n",
        "  def forward(self, x):\n",
        "    x = self.dwc(x)\n",
        "    x = x.view(-1, self.num_capsules, self.dim_capsules) #output shape is (batch_size, 16, 8)\n",
        "    x = self.squash(x)\n",
        "    return x\n",
        "\n",
        "  def squash(self, x):\n",
        "    eps = 1e-20\n",
        "    norm = torch.linalg.norm(x, ord = 2, dim = -1, keepdim = True)\n",
        "    coef = 1 - 1 / (torch.exp(norm) + eps)\n",
        "    unit = x / (norm + eps)\n",
        "    return coef * unit\n",
        "\n",
        "class FCCaps(nn.Module):\n",
        "  def __init__(self, num_capsules = 10, dim_capsules = 16, kernel_iniitalizer = 'he_normal', input_shape = (batch_size, 16, 8)):\n",
        "    super(FCCaps, self).__init__()\n",
        "    self.num_capsules = num_capsules\n",
        "    self.dim_capsules = dim_capsules\n",
        "    input_num_cap = input_shape[-2]\n",
        "    input_dim_cap = input_shape[-1]\n",
        "    self.W = nn.Parameter(torch.randn(self.num_capsules, input_num_cap, input_dim_cap, self.dim_capsules)) #self.N, input_N, input_D, self.D\n",
        "    nn.init.kaiming_normal_(self.W, mode = 'fan_in', nonlinearity = 'relu')\n",
        "\n",
        "    self.b = nn.Parameter(torch.zeros(self.num_capsules, input_num_cap, 1))\n",
        "\n",
        "    #W = torch.cat([self.W] * batch_size, dim=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    u = torch.einsum('...ji,kjiz->...kjz', x, self.W)\n",
        "\n",
        "    # c shape=(None, N, H*W*input_N, 1)\n",
        "    c = torch.einsum('...ij,...kj->...i', u, u).unsqueeze(-1)\n",
        "    c = c / torch.sqrt(torch.tensor(self.dim_capsules, dtype=torch.float32))\n",
        "    c = F.softmax(c, dim=1)  # c shape=(None, N, H*W*input_N, 1)\n",
        "    c = c + self.b\n",
        "\n",
        "    # s shape=(None, N, D)\n",
        "    s = torch.sum(u * c, dim=-2) #(batch_size, self.num_capsules, self.dim_capsules)\n",
        "\n",
        "    # v shape=(None, N, D)\n",
        "    v = self.squash(s)\n",
        "\n",
        "    return v\n",
        "\n",
        "  def squash(self, x):\n",
        "    eps = 1e-20\n",
        "    norm = torch.linalg.norm(x, ord = 2, dim = -1, keepdim = True)\n",
        "    coef = 1 - 1 / (torch.exp(norm) + eps)\n",
        "    unit = x / (norm + eps)\n",
        "    return coef * unit\n",
        "\n",
        "\n",
        "class CapsMask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CapsMask, self).__init__()\n",
        "\n",
        "    def forward(self, x, y_true=None):\n",
        "        if y_true is not None:  # training mode\n",
        "            mask = y_true\n",
        "        else:  # testing mode\n",
        "            # convert list of maximum value's indices to one-hot tensor\n",
        "            temp = torch.sqrt(torch.sum(x**2, dim=-1))\n",
        "            mask = F.one_hot(torch.argmax(temp, dim=1), num_classes=temp.shape[1])\n",
        "\n",
        "        masked = x * mask.unsqueeze(-1)\n",
        "        return masked.view(x.shape[0], -1)  # reshape\n",
        "\n",
        "class EfficientCapsNet(nn.Module):\n",
        "  def __init__(self, input_size = (3, 224, 224)):\n",
        "    super(EfficientCapsNet, self).__init__()\n",
        "    self.convlayer = ConvLayer()\n",
        "    self.primarycaps = DepthWiseConv()\n",
        "    self.routingcaps = FCCaps()\n",
        "    #self.len_final_caps = CapsLen()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convlayer(x)\n",
        "    x = self.primarycaps(x)\n",
        "    x = self.routingcaps(x)\n",
        "    return x#, self.len_final_caps(x)\n",
        "\n",
        "class ReconstructionNet(nn.Module):\n",
        "  def __init__(self, input_size = (3, 224, 224), num_classes = 2, num_capsules = 16):\n",
        "    super(ReconstructionNet, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.decoder = nn.Sequential(\n",
        "            nn.Linear(16 * 10, 512 * 7 * 7),  # Fully connected layer to reshape to a small feature map\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 7, 7)),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # To ensure the output pixel values are between 0 and 1\n",
        "        )\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.decoder:\n",
        "      if isinstance(layer, nn.Linear):\n",
        "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if layer.bias is not None:\n",
        "          nn.init.constant_(layer.bias, 0)\n",
        "      elif isinstance(layer, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if layer.bias is not None:\n",
        "          nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "class FinalCapsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FinalCapsNet, self).__init__()\n",
        "        self.efficient_capsnet = EfficientCapsNet()\n",
        "        self.mask = CapsMask()\n",
        "        self.generator = ReconstructionNet()\n",
        "\n",
        "    def forward(self, x, y_true=None, mode='train'):\n",
        "        x = self.efficient_capsnet(x)\n",
        "        if mode == \"train\":\n",
        "            masked = self.mask(x, y_true)\n",
        "        elif mode == \"eval\":\n",
        "            masked = self.mask(x)\n",
        "        x = self.generator(masked)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zg1YXutoNHf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarginLoss(nn.Module):\n",
        "    def __init__(self, m_pos=0.9, m_neg=0.1, lambda_=0.5):\n",
        "        super(MarginLoss, self).__init__()\n",
        "        self.m_pos = m_pos\n",
        "        self.m_neg = m_neg\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def forward(self, targets, digit_probs):\n",
        "        assert targets.shape is not digit_probs.shape\n",
        "        present_losses = (\n",
        "            targets * torch.clamp_min(self.m_pos - digit_probs, min=0.0) ** 2\n",
        "        )\n",
        "        absent_losses = (1 - targets) * torch.clamp_min(\n",
        "            digit_probs - self.m_neg, min=0.0\n",
        "        ) ** 2\n",
        "        losses = present_losses + self.lambda_ * absent_losses\n",
        "        return torch.mean(torch.sum(losses, dim=1))\n",
        "\n",
        "\n",
        "class ReconstructionLoss(nn.Module):\n",
        "    def forward(self, reconstructions, input_images):\n",
        "        return torch.nn.MSELoss(reduction=\"mean\")(reconstructions, input_images)\n",
        "\n",
        "\n",
        "class TotalLoss(nn.Module):\n",
        "    def __init__(self, m_pos=0.9, m_neg=0.1, lambda_=0.5, recon_factor=0.0005):\n",
        "        super(TotalLoss, self).__init__()\n",
        "        self.margin_loss = MarginLoss(m_pos, m_neg, lambda_)\n",
        "        self.recon_loss = ReconstructionLoss()\n",
        "        self.recon_factor = recon_factor\n",
        "\n",
        "    def forward(self, input_images, targets, reconstructions, digit_probs):\n",
        "        margin = self.margin_loss(targets, digit_probs)\n",
        "        recon = self.recon_loss(reconstructions, input_images)\n",
        "        return margin + self.recon_factor * recon\n"
      ],
      "metadata": {
        "id": "tVAWu33Btqf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ECN = FinalCapsNet()"
      ],
      "metadata": {
        "id": "lwTfxQSPJLB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchInfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppyPK2toxsbR",
        "outputId": "e0c15e9d-cc58-4e9d-c7d5-65e550db732f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchInfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchInfo\n",
            "Successfully installed torchInfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(\n",
        "    ECN,\n",
        "    input_size = (17,3,224,224),\n",
        "    verbose=0,\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1mLrKztw9YR",
        "outputId": "af4c5481-1965-4d49-b856-41d34a830869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=============================================================================================================================\n",
              "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
              "=============================================================================================================================\n",
              "FinalCapsNet (FinalCapsNet)                   [17, 3, 224, 224]    [17, 3, 112, 112]    --                   True\n",
              "├─EfficientCapsNet (efficient_capsnet)        [17, 3, 224, 224]    [17, 10, 16]         --                   True\n",
              "│    └─ConvLayer (convlayer)                  [17, 3, 224, 224]    [17, 128, 11, 11]    --                   True\n",
              "│    │    └─Conv2d (conv1)                    [17, 3, 224, 224]    [17, 32, 109, 109]   4,736                True\n",
              "│    │    └─BatchNorm2d (bn1)                 [17, 32, 109, 109]   [17, 32, 109, 109]   64                   True\n",
              "│    │    └─Conv2d (conv2)                    [17, 32, 109, 109]   [17, 64, 53, 53]     51,264               True\n",
              "│    │    └─BatchNorm2d (bn2)                 [17, 64, 53, 53]     [17, 64, 53, 53]     128                  True\n",
              "│    │    └─Conv2d (conv3)                    [17, 64, 53, 53]     [17, 64, 25, 25]     102,464              True\n",
              "│    │    └─BatchNorm2d (bn3)                 [17, 64, 25, 25]     [17, 64, 25, 25]     128                  True\n",
              "│    │    └─Conv2d (conv4)                    [17, 64, 25, 25]     [17, 128, 11, 11]    204,928              True\n",
              "│    │    └─BatchNorm2d (bn4)                 [17, 128, 11, 11]    [17, 128, 11, 11]    256                  True\n",
              "│    └─DepthWiseConv (primarycaps)            [17, 128, 11, 11]    [17, 16, 8]          --                   True\n",
              "│    │    └─Conv2d (dwc)                      [17, 128, 11, 11]    [17, 128, 1, 1]      15,616               True\n",
              "│    └─FCCaps (routingcaps)                   [17, 16, 8]          [17, 10, 16]         20,640               True\n",
              "├─CapsMask (mask)                             [17, 10, 16]         [17, 160]            --                   --\n",
              "├─ReconstructionNet (generator)               [17, 160]            [17, 3, 112, 112]    --                   True\n",
              "│    └─Sequential (decoder)                   [17, 160]            [17, 3, 112, 112]    --                   True\n",
              "│    │    └─Linear (0)                        [17, 160]            [17, 25088]          4,039,168            True\n",
              "│    │    └─ReLU (1)                          [17, 25088]          [17, 25088]          --                   --\n",
              "│    │    └─Unflatten (2)                     [17, 25088]          [17, 512, 7, 7]      --                   --\n",
              "│    │    └─ConvTranspose2d (3)               [17, 512, 7, 7]      [17, 256, 14, 14]    2,097,408            True\n",
              "│    │    └─ReLU (4)                          [17, 256, 14, 14]    [17, 256, 14, 14]    --                   --\n",
              "│    │    └─ConvTranspose2d (5)               [17, 256, 14, 14]    [17, 128, 28, 28]    524,416              True\n",
              "│    │    └─ReLU (6)                          [17, 128, 28, 28]    [17, 128, 28, 28]    --                   --\n",
              "│    │    └─ConvTranspose2d (7)               [17, 128, 28, 28]    [17, 64, 56, 56]     131,136              True\n",
              "│    │    └─ReLU (8)                          [17, 64, 56, 56]     [17, 64, 56, 56]     --                   --\n",
              "│    │    └─ConvTranspose2d (9)               [17, 64, 56, 56]     [17, 3, 112, 112]    3,075                True\n",
              "│    │    └─Sigmoid (10)                      [17, 3, 112, 112]    [17, 3, 112, 112]    --                   --\n",
              "=============================================================================================================================\n",
              "Total params: 7,195,427\n",
              "Trainable params: 7,195,427\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 26.61\n",
              "=============================================================================================================================\n",
              "Input size (MB): 10.24\n",
              "Forward/backward pass size (MB): 223.74\n",
              "Params size (MB): 28.78\n",
              "Estimated Total Size (MB): 262.76\n",
              "============================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEoita450hgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}